# -*- coding: utf-8 -*-
"""unigram.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QgdTa1iHlGs6niU0J5CLap3lEtaBa1jC
"""



!pip uninstall -y nltk
!pip install --no-cache-dir nltk

import os
import collections
import re
import nltk
import multiprocessing
from tqdm import tqdm
from joblib import Parallel, delayed
from google.colab import files
import nltk
nltk.download('punkt', force=True)
nltk.download('punkt_tab')

import nltk
nltk.download('punkt', force=True)
nltk.download('punkt_tab')

#nltk.download('punkt')
nltk.data.path.append("/usr/local/share/nltk_data")

sents_file = "/content/postags.txt"
output_file = "/content/unigram.txt"

def load_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.readlines()

def process_text(text):
    text = text.strip().lower()
    words = nltk.word_tokenize(text)
    words = [word for word in words if re.match(r'^[a-zA-Z0-9]+$', word)]
    return words

lines = load_text_file(sents_file)
num_cores = multiprocessing.cpu_count()
processed_tokens = Parallel(n_jobs=num_cores)(
    delayed(process_text)(line) for line in tqdm(lines, desc="Processing dataset")
)

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

with open(output_file, "w", encoding="utf-8") as f:
    for word, freq in tqdm(unigram_counts.items(), desc="Writing output"):
        probability = freq / total_tokens
        f.write(f"{word}\t{probability}\n")

files.download(output_file)

#nltk.download('punkt')
nltk.data.path.append("/usr/local/share/nltk_data")

sents_file = "/content/postags.txt"
output_file = "/content/unigram.txt"

def load_text_file(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.readlines()

def process_text(text):
    text = text.strip().lower()
    words = nltk.word_tokenize(text)
    words = [word for word in words if re.match(r'^[a-zA-Z0-9]+$', word)]
    return words

lines = load_text_file(sents_file)
num_cores = multiprocessing.cpu_count()
processed_tokens = Parallel(n_jobs=num_cores)(
    delayed(process_text)(line) for line in tqdm(lines, desc="Processing dataset")
)

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

with open(output_file, "w", encoding="utf-8") as f:
    for word, freq in tqdm(unigram_counts.items(), desc="Writing output"):
        probability = freq / total_tokens
        f.write(f"{word}\t{probability}\n")

files.download(output_file)

!pip install datasets

## NNS removal
import collections
import re
import nltk
import multiprocessing
from datasets import load_dataset
from google.colab import files
from tqdm import tqdm
from joblib import Parallel, delayed

nltk.download('punkt')
nltk.data.path.append("/usr/local/share/nltk_data")

from datasets import load_dataset

dataset_name = "kanishka/counterfactual_babylm_aann_indef_articles_with_pl_nouns_removal_new"
output_file = "/content/indef_unigram.txt"

dataset = load_dataset(dataset_name, split="train")

def process_text(text):
    text = text.strip().lower()
    words = nltk.word_tokenize(text)
    words = [word for word in words if re.match(r'^[a-zA-Z0-9]+$', word)]
    return words

num_cores = multiprocessing.cpu_count()
processed_tokens = Parallel(n_jobs=num_cores)(
    delayed(process_text)(item['text']) for item in tqdm(dataset, desc="Processing dataset")
)

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

with open(output_file, "w", encoding="utf-8") as f:
    for word, freq in tqdm(unigram_counts.items(), desc="Writing output"):
        probability = freq / total_tokens
        f.write(f"{word}\t{probability}\n")

files.download(output_file)

import nltk
import re
import collections
import multiprocessing
import zipfile
import os
import pyarrow.ipc as ipc
import pyarrow as pa
from joblib import Parallel, delayed
from tqdm import tqdm
from google.colab import files

# Ensure NLTK has the necessary tokenization data
nltk.data.path.append("/usr/local/share/nltk_data")

zip_file_path = "/content/dataset_without_aann_with_pos_tags.zip"
extracted_folder = "/content/dataset_without_aann_with_pos_tags"
output_file = "/content/unigram_without_aann.txt"

# Extract ZIP file
def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

# Locate .arrow files in train set
train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

# Function to read .arrow files using Arrow IPC format
def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

# Load and process data
all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_text(text):
    text = text.strip().lower()
    words = nltk.word_tokenize(text)
    words = [word for word in words if re.match(r'^[a-zA-Z0-9]+$', word)]
    return words

num_cores = multiprocessing.cpu_count()
processed_tokens = Parallel(n_jobs=num_cores)(
    delayed(process_text)(text) for text in tqdm(all_texts, desc="Processing dataset")
)

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

# Compute unigram probabilities
unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

# Write to output file
with open(output_file, "w", encoding="utf-8") as f:
    for word, freq in tqdm(unigram_counts.items(), desc="Writing output"):
        probability = freq / total_tokens
        f.write(f"{word}\t{probability}\n")

# Download output file
files.download(output_file)

import nltk
import re
import collections
import multiprocessing
import zipfile
import os
import pyarrow.ipc as ipc
import pyarrow as pa
from joblib import Parallel, delayed
from tqdm import tqdm
from google.colab import files

# Ensure NLTK has the necessary tokenization data
nltk.data.path.append("/usr/local/share/nltk_data")

zip_file_path = "/content/unablated_dataset_pos_tags.zip"
extracted_folder = "/content/unablated_dataset_pos_tags"
output_file = "/content/unigram_unablated.txt"

# Extract ZIP file
def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

# Locate .arrow files in train set
train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

# Function to read .arrow files using Arrow IPC format
def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

# Load and process data
all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_text(text):
    text = text.strip().lower()
    words = nltk.word_tokenize(text)
    words = [word for word in words if re.match(r'^[a-zA-Z0-9]+$', word)]
    return words

num_cores = multiprocessing.cpu_count()
processed_tokens = Parallel(n_jobs=num_cores)(
    delayed(process_text)(text) for text in tqdm(all_texts, desc="Processing dataset")
)

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

# Compute unigram probabilities
unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

# Write to output file
with open(output_file, "w", encoding="utf-8") as f:
    for word, freq in tqdm(unigram_counts.items(), desc="Writing output"):
        probability = freq / total_tokens
        f.write(f"{word}\t{probability}\n")

# Download output file
files.download(output_file)

#GPU
import os
import zipfile
import multiprocessing
import collections
import pyarrow.ipc as ipc
import pyarrow as pa
import torch
import spacy
import numpy as np
from tqdm import tqdm
from google.colab import files

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
nlp.tokenizer.use_heuristic_tokenizer = True

zip_file_path = "/content/dataset_without_aann_with_pos_tags.zip"
extracted_folder = "/content/dataset_without_aann_with_pos_tags"
output_file = "/content/unigram_without_aann.txt"

def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_text(text):
    doc = nlp(text.lower().strip())
    words = [token.text for token in doc if token.is_alpha or token.is_digit]
    return words

num_cores = multiprocessing.cpu_count()
with multiprocessing.Pool(num_cores) as pool:
    processed_tokens = list(tqdm(pool.imap(process_text, all_texts), total=len(all_texts), desc="Tokenizing"))

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(word) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
counts = counts.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

#GPU
import os
import zipfile
import multiprocessing
import collections
import pyarrow.ipc as ipc
import pyarrow as pa
import torch
import spacy
import numpy as np
from tqdm import tqdm
from google.colab import files

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
nlp.tokenizer.use_heuristic_tokenizer = True

zip_file_path = "/content/unablated_dataset_pos_tags.zip"
extracted_folder = "/content/unablated_dataset_pos_tags"
output_file = "/content/unigram_unablated.txt"

def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_text(text):
    doc = nlp(text.lower().strip())
    words = [token.text for token in doc if token.is_alpha or token.is_digit]
    return words

num_cores = multiprocessing.cpu_count()
with multiprocessing.Pool(num_cores) as pool:
    processed_tokens = list(tqdm(pool.imap(process_text, all_texts), total=len(all_texts), desc="Tokenizing"))

tokens = [word for sublist in processed_tokens for word in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(word) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
counts = counts.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)



import os
import zipfile
import pyarrow.ipc as ipc
import pyarrow as pa
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from transformers import AlbertTokenizerFast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AlbertTokenizerFast.from_pretrained("albert-base-v2")

zip_file_path = "/content/dataset_without_aann_with_pos_tags.zip"
extracted_folder = "/content/dataset_without_aann_with_pos_tags"
output_file = "/content/unigram_without_aann.txt"

def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_texts(texts):
    return tokenizer.batch_encode_plus(
        texts,
        truncation=True,
        max_length=512,
        add_special_tokens=False
    )["input_ids"]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(all_texts), batch_size), desc="Tokenizing in Batches"):
    batch = all_texts[i:i + batch_size]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(str(word)) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
counts = counts.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

import os
import zipfile
import pyarrow.ipc as ipc
import pyarrow as pa
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from transformers import AlbertTokenizerFast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AlbertTokenizerFast.from_pretrained("albert-base-v2")

zip_file_path = "/content/unablated_dataset_pos_tags.zip"
extracted_folder = "/content/unablated_dataset_pos_tags"
output_file = "/content/unigram_unablated.txt"

def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_texts(texts):
    return tokenizer.batch_encode_plus(
        texts,
        truncation=True,
        max_length=512,
        add_special_tokens=False
    )["input_ids"]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(all_texts), batch_size), desc="Tokenizing in Batches"):
    batch = all_texts[i:i + batch_size]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(str(word)) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
counts = counts.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

import collections
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from transformers import AlbertTokenizerFast
from datasets import load_dataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AlbertTokenizerFast.from_pretrained("albert-base-v2")

dataset_name = "kanishka/counterfactual_babylm_aann_indef_articles_with_pl_nouns_removal_new"
output_file = "/content/unigram_indef.txt"

dataset = load_dataset(dataset_name, split="train")

def process_texts(texts):
    return tokenizer.batch_encode_plus(
        texts,
        truncation=True,
        max_length=512,
        add_special_tokens=False
    )["input_ids"]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(dataset), batch_size), desc="Tokenizing in Batches"):
    batch = [item["text"].lower().strip() for item in dataset.select(range(i, min(i + batch_size, len(dataset))))]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(str(word)) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

import os
import zipfile
import pyarrow.ipc as ipc
import pyarrow as pa
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from transformers import AlbertTokenizerFast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AlbertTokenizerFast.from_pretrained("albert-base-v2")

zip_file_path = "/content/unablated_dataset_pos_tags.zip"
extracted_folder = "/content/unablated_dataset_pos_tags"
output_file = "/content/unigram_morph.txt"

def extract_zip(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_to)

extract_zip(zip_file_path, extracted_folder)

train_folder = os.path.join(extracted_folder, "train")
arrow_files = [os.path.join(train_folder, f) for f in os.listdir(train_folder) if f.endswith(".arrow")]

def read_arrow_file(file_path):
    text_data = []
    with pa.memory_map(file_path, "r") as source:
        reader = ipc.open_stream(source)
        for batch in reader:
            table = batch.to_pandas()
            if "text" in table.columns:
                text_data.extend(table["text"].dropna().astype(str).tolist())
    return text_data

all_texts = []
for arrow_file in arrow_files:
    all_texts.extend(read_arrow_file(arrow_file))

def process_texts(texts):
    return tokenizer.batch_encode_plus(
        texts,
        truncation=True,
        max_length=512,
        add_special_tokens=False
    )["input_ids"]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(all_texts), batch_size), desc="Tokenizing in Batches"):
    batch = all_texts[i:i + batch_size]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

token_tensor = torch.tensor([hash(str(word)) for word in tokens], device=device)

unique_tokens, counts = torch.unique(token_tensor, return_counts=True)
total_tokens = counts.sum().item()
unigram_probs = counts.float() / total_tokens

unique_tokens = unique_tokens.cpu().numpy()
unigram_probs = unigram_probs.cpu().numpy()

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(zip(unique_tokens, unigram_probs), total=len(unique_tokens), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

import collections
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from transformers import AlbertTokenizerFast

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AlbertTokenizerFast.from_pretrained("albert-base-v2")

input_file = "/content/train.sents"
output_file = "/content/unigram_morph.txt"

with open(input_file, "r", encoding="utf-8") as f:
    all_texts = [line.strip() for line in f.readlines() if line.strip()]

def process_texts(texts):
    return tokenizer.batch_encode_plus(
        texts,
        truncation=True,
        max_length=512,
        add_special_tokens=False
    )["input_ids"]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(all_texts), batch_size), desc="Tokenizing in Batches"):
    batch = all_texts[i:i + batch_size]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

unigram_probs = {word: count / total_tokens for word, count in unigram_counts.items()}

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(unigram_probs.items(), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

sample_text = "This is an example sentence."

encoded_sample = tokenizer.encode_plus(sample_text, truncation=True, max_length=512, add_special_tokens=True)

print("Tokens:", tokenizer.convert_ids_to_tokens(encoded_sample["input_ids"]))
print("Token IDs:", encoded_sample["input_ids"])


print("\nðŸ”¹ **Decoded Text:**")
print(tokenizer.decode(encoded_sample.ids))

print("\nðŸ”¹ **Unigram Model Samples:**")
for i, (word, prob) in enumerate(unigram_probs.items()):
    print(f"{word}: {prob:.6f}")
    if i == 9:
        break  # Print only the first 10 words

from tokenizers import Tokenizer

tokenizer = Tokenizer.from_file("/content/normal-tokenizer.json")

text = "This is an example sentence."

encoded = tokenizer.encode(text)
print("Tokenized Output:", encoded.tokens)

print("Token IDs:", encoded.ids)

decoded_text = tokenizer.decode(encoded.ids)
print("Decoded Text:", decoded_text)

import collections
import torch
import numpy as np
from tqdm import tqdm
from google.colab import files
from tokenizers import Tokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = Tokenizer.from_file("/content/normal-tokenizer.json")

input_file = "/content/train.sents"
output_file = "/content/unigram_model.txt"

with open(input_file, "r", encoding="utf-8") as f:
    all_texts = [line.strip() for line in f.readlines() if line.strip()]

def process_texts(texts):
    return [tokenizer.encode(text).tokens for text in texts]

batch_size = 1000
processed_tokens = []
for i in tqdm(range(0, len(all_texts), batch_size), desc="Tokenizing in Batches"):
    batch = all_texts[i:i + batch_size]
    processed_tokens.extend(process_texts(batch))

tokens = [token for sublist in processed_tokens for token in sublist]
if not tokens:
    raise ValueError("No tokens were extracted. Check your dataset!")

unigram_counts = collections.Counter(tokens)
total_tokens = sum(unigram_counts.values())

unigram_probs = {word: count / total_tokens for word, count in unigram_counts.items()}

with open(output_file, "w", encoding="utf-8") as f:
    for word, prob in tqdm(unigram_probs.items(), desc="Writing output"):
        f.write(f"{word}\t{prob}\n")

files.download(output_file)

sample_text = "This is an example sentence."
encoded_sample = tokenizer.encode(sample_text)

print("\nðŸ”¹ **Example Tokenization Output:**")
print("Tokens:", encoded_sample.tokens)
print("Token IDs:", encoded_sample.ids)

print("\nðŸ”¹ **Decoded Text:**")
print(tokenizer.decode(encoded_sample.ids))

print("\nðŸ”¹ **Unigram Model Samples:**")
for i, (word, prob) in enumerate(unigram_probs.items()):
    print(f"{word}: {prob:.6f}")
    if i == 9:
        break

sample_text = "I went to MOD pizza."
encoded_sample = tokenizer.encode(sample_text)

print("\nðŸ”¹ **Example Tokenization Output:**")
print("Tokens:", encoded_sample.tokens)
print("Token IDs:", encoded_sample.ids)

print("\nðŸ”¹ **Decoded Text:**")
print(tokenizer.decode(encoded_sample.ids))

print("\nðŸ”¹ **Unigram Model Samples:**")
for i, (word, prob) in enumerate(unigram_probs.items()):
    print(f"{word}: {prob:.6f}")
    if i == 80:
        break